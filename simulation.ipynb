{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get started with Network Sciecne Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all required libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Parameters:\n",
    "- **NUM_AGENTS**: Number of agents in the network. This controls the size of the simulation.\n",
    "- **INITIAL_COOPERATION_PROB**: The probability that an agent starts as a cooperator.\n",
    "- **INEQUALITY_LEVEL**: Controls the inequality of resource distribution using a Pareto distribution. Higher values mean more inequality.\n",
    "- **NUM_TIME_STEPS**: The number of time steps over which the simulation runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_AGENTS = 100\n",
    "INITIAL_COOPERATION_PROB = 0.5\n",
    "INEQUALITY_LEVEL = 2.0\n",
    "NUM_TIME_STEPS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters of Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payoff Matrix (T > R > P > S)\n",
    "# T: Temptation to defect\n",
    "# R: Reward for mutual cooperation\n",
    "# P: Punishment for mutual defection\n",
    "# S: Sucker's payoff\n",
    "T = 5\n",
    "R = 3\n",
    "P = 2\n",
    "S = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def assign_resources(G, inequality_level):\n",
    "    # Use Pareto distribution to assign resources (wealth)\n",
    "    # Higher inequality_level means more inequality\n",
    "    m = 1  # Scale parameter\n",
    "    resources = (np.random.pareto(inequality_level, NUM_AGENTS) + 1) * m\n",
    "    total_resources = np.sum(resources)\n",
    "    normalized_resources = resources / total_resources  # Normalize to sum to 1\n",
    "    for i, node in enumerate(G.nodes()):\n",
    "        G.nodes[node]['resource'] = normalized_resources[i]\n",
    "\n",
    "def initialize_strategies(G, cooperation_prob):\n",
    "    for node in G.nodes():\n",
    "        if random.random() < cooperation_prob:\n",
    "            G.nodes[node]['strategy'] = 'C'\n",
    "        else:\n",
    "            G.nodes[node]['strategy'] = 'D'\n",
    "\n",
    "def play_game(G):\n",
    "    payoffs = {}\n",
    "    for node in G.nodes():\n",
    "        strategy = G.nodes[node]['strategy']\n",
    "        resource = G.nodes[node]['resource']\n",
    "        payoff = 0\n",
    "        for neighbor in G.neighbors(node):\n",
    "            neighbor_strategy = G.nodes[neighbor]['strategy']\n",
    "            # Adjust payoffs based on own resource level (optional complexity)\n",
    "            if strategy == 'C' and neighbor_strategy == 'C':\n",
    "                payoff += R\n",
    "            elif strategy == 'C' and neighbor_strategy == 'D':\n",
    "                payoff += S\n",
    "            elif strategy == 'D' and neighbor_strategy == 'C':\n",
    "                payoff += T\n",
    "            elif strategy == 'D' and neighbor_strategy == 'D':\n",
    "                payoff += P\n",
    "        # Total payoff adjusted by own resource level\n",
    "        payoffs[node] = payoff * resource\n",
    "    return payoffs\n",
    "\n",
    "def update_strategies(G, payoffs):\n",
    "    new_strategies = {}\n",
    "    for node in G.nodes():\n",
    "        # Select a random neighbor\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        if neighbors:\n",
    "            neighbor = random.choice(neighbors)\n",
    "            # Compare payoffs\n",
    "            if payoffs[neighbor] > payoffs[node]:\n",
    "                # Adopt neighbor's strategy with probability proportional to payoff difference\n",
    "                prob = (payoffs[neighbor] - payoffs[node]) / (max(payoffs.values()) - min(payoffs.values()) + 1e-6)\n",
    "                if random.random() < prob:\n",
    "                    new_strategies[node] = G.nodes[neighbor]['strategy']\n",
    "                else:\n",
    "                    new_strategies[node] = G.nodes[node]['strategy']\n",
    "            else:\n",
    "                new_strategies[node] = G.nodes[node]['strategy']\n",
    "        else:\n",
    "            new_strategies[node] = G.nodes[node]['strategy']\n",
    "    # Update strategies\n",
    "    for node in G.nodes():\n",
    "        G.nodes[node]['strategy'] = new_strategies[node]\n",
    "\n",
    "def gini_coefficient(resources):\n",
    "    sorted_resources = sorted(resources)\n",
    "    n = len(resources)\n",
    "    cumulative = np.cumsum(sorted_resources)\n",
    "    gini_index = (2 * np.sum(cumulative)) / (n * np.sum(sorted_resources)) - (n + 1) / n\n",
    "    return gini_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate():\n",
    "    G = nx.erdos_renyi_graph(NUM_AGENTS, 0.1)  # Example of a random network\n",
    "    \n",
    "    assign_resources(G, INEQUALITY_LEVEL)\n",
    "    initialize_strategies(G, INITIAL_COOPERATION_PROB)\n",
    "    \n",
    "    cooperation_levels = []\n",
    "    resource_distribution = [G.nodes[node]['resource'] for node in G.nodes()]\n",
    "\n",
    "    for t in range(NUM_TIME_STEPS):\n",
    "        payoffs = play_game(G)\n",
    "        update_strategies(G, payoffs)\n",
    "        \n",
    "        # Measure cooperation level\n",
    "        num_cooperators = sum(1 for node in G.nodes() if G.nodes[node]['strategy'] == 'C')\n",
    "        cooperation_level = num_cooperators / NUM_AGENTS\n",
    "        cooperation_levels.append(cooperation_level)\n",
    "        #print(f\"Time Step {t+1}: Cooperation Level = {cooperation_level:.2f}\")\n",
    "\n",
    "    return cooperation_levels, resource_distribution, G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(cooperation_levels, resource_distribution, G):\n",
    "    # Plot cooperation levels over time\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(NUM_TIME_STEPS), cooperation_levels, marker='o')\n",
    "    plt.title('Cooperation Level Over Time')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Cooperation Level')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot resource distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(resource_distribution, bins=20, edgecolor='black')\n",
    "    plt.title('Resource Distribution Among Agents')\n",
    "    plt.xlabel('Normalized Resource Level')\n",
    "    plt.ylabel('Number of Agents')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Visualize network with strategies\n",
    "    color_map = {'C': 'blue', 'D': 'red'}\n",
    "    node_colors = [color_map[G.nodes[node]['strategy']] for node in G.nodes()]\n",
    "    sizes = [G.nodes[node]['resource'] * 1000 for node in G.nodes()]  # Scale sizes for visualization\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    nx.draw_networkx(G, node_color=node_colors, node_size=sizes, with_labels=False)\n",
    "    plt.title('Network Visualization: Blue = Cooperators, Red = Defectors')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Resource Distribution vs Cooperation Level\n",
    "    strategies = [G.nodes[node]['strategy'] for node in G.nodes()]\n",
    "    cooperation_flags = [1 if strategy == 'C' else 0 for strategy in strategies]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(resource_distribution, cooperation_flags, c=cooperation_flags, cmap='coolwarm', alpha=0.7)\n",
    "    plt.title('Resource Distribution vs Cooperation Level')\n",
    "    plt.xlabel('Resource Level')\n",
    "    plt.ylabel('Cooperation (1 = Cooperate, 0 = Defect)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Gini Coefficient vs Cooperation Level\n",
    "    gini = gini_coefficient(resource_distribution)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(['Cooperation Level', 'Gini Coefficient'], [cooperation_levels[-1], gini])\n",
    "    plt.title('Final Cooperation Level vs Gini Coefficient (Inequality)')\n",
    "    plt.ylabel('Value')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16dde0e3e6bd4074876f91910dff754f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=100, description='num_agents', max=500, min=50, step=50), FloatSlider(vaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interactive_simulation(num_agents, cooperation_prob, inequality_level)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def interactive_simulation(num_agents, cooperation_prob, inequality_level):\n",
    "    global NUM_AGENTS, INITIAL_COOPERATION_PROB, INEQUALITY_LEVEL\n",
    "    NUM_AGENTS = num_agents\n",
    "    INITIAL_COOPERATION_PROB = cooperation_prob\n",
    "    INEQUALITY_LEVEL = inequality_level\n",
    "\n",
    "    #Run & plot simulation\n",
    "    cooperation_levels, resource_distribution, G = simulate()\n",
    "    plot_results(cooperation_levels, resource_distribution, G)\n",
    "\n",
    "widgets.interact(interactive_simulation,\n",
    "                 num_agents=widgets.IntSlider(min=50, max=500, step=50, value=100),\n",
    "                 cooperation_prob=widgets.FloatSlider(min=0, max=1, step=0.1, value=0.5),\n",
    "                 inequality_level=widgets.FloatSlider(min=1, max=5, step=0.1, value=2.0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envNS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
