{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get started with Network Sciecne Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all required libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Parameters:\n",
    "- **NUM_AGENTS**: Number of agents in the network. This controls the size of the simulation.\n",
    "- **INITIAL_COOPERATION_PROB**: The probability that an agent starts as a cooperator.\n",
    "- **INEQUALITY_LEVEL**: Controls the inequality of resource distribution using a Pareto distribution. Higher values mean more inequality.\n",
    "- **NUM_TIME_STEPS**: The number of time steps over which the simulation runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_AGENTS = 100\n",
    "INITIAL_COOPERATION_PROB = 0.5\n",
    "INEQUALITY_LEVEL = 2.0\n",
    "NUM_TIME_STEPS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters of Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payoff Matrix (T > R > P > S)\n",
    "# T: Temptation to defect\n",
    "# R: Reward for mutual cooperation\n",
    "# P: Punishment for mutual defection\n",
    "# S: Sucker's payoff\n",
    "T = 5\n",
    "R = 3\n",
    "P = 2\n",
    "S = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def assign_resources(G, inequality_level):\n",
    "    # Use Pareto distribution to assign resources (wealth)\n",
    "    # Higher inequality_level means more inequality\n",
    "    m = 1  # Scale parameter\n",
    "    resources = (np.random.pareto(inequality_level, NUM_AGENTS) + 1) * m\n",
    "    total_resources = np.sum(resources)\n",
    "    normalized_resources = resources / total_resources  # Normalize to sum to 1\n",
    "    for i, node in enumerate(G.nodes()):\n",
    "        G.nodes[node]['resource'] = normalized_resources[i]\n",
    "\n",
    "def initialize_strategies(G, cooperation_prob):\n",
    "    for node in G.nodes():\n",
    "        if random.random() < cooperation_prob:\n",
    "            G.nodes[node]['strategy'] = 'C'\n",
    "        else:\n",
    "            G.nodes[node]['strategy'] = 'D'\n",
    "\n",
    "def play_game(G):\n",
    "    payoffs = {}\n",
    "    for node in G.nodes():\n",
    "        strategy = G.nodes[node]['strategy']\n",
    "        resource = G.nodes[node]['resource']\n",
    "        payoff = 0\n",
    "        for neighbor in G.neighbors(node):\n",
    "            neighbor_strategy = G.nodes[neighbor]['strategy']\n",
    "            # Adjust payoffs based on own resource level (optional complexity)\n",
    "            if strategy == 'C' and neighbor_strategy == 'C':\n",
    "                payoff += R\n",
    "            elif strategy == 'C' and neighbor_strategy == 'D':\n",
    "                payoff += S\n",
    "            elif strategy == 'D' and neighbor_strategy == 'C':\n",
    "                payoff += T\n",
    "            elif strategy == 'D' and neighbor_strategy == 'D':\n",
    "                payoff += P\n",
    "        # Total payoff adjusted by own resource level\n",
    "        payoffs[node] = payoff * resource\n",
    "    return payoffs\n",
    "\n",
    "def update_strategies(G, payoffs):\n",
    "    new_strategies = {}\n",
    "    for node in G.nodes():\n",
    "        # Select a random neighbor\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        if neighbors:\n",
    "            neighbor = random.choice(neighbors)\n",
    "            # Compare payoffs\n",
    "            if payoffs[neighbor] > payoffs[node]:\n",
    "                # Adopt neighbor's strategy with probability proportional to payoff difference\n",
    "                prob = (payoffs[neighbor] - payoffs[node]) / (max(payoffs.values()) - min(payoffs.values()) + 1e-6)\n",
    "                if random.random() < prob:\n",
    "                    new_strategies[node] = G.nodes[neighbor]['strategy']\n",
    "                else:\n",
    "                    new_strategies[node] = G.nodes[node]['strategy']\n",
    "            else:\n",
    "                new_strategies[node] = G.nodes[node]['strategy']\n",
    "        else:\n",
    "            new_strategies[node] = G.nodes[node]['strategy']\n",
    "    # Update strategies\n",
    "    for node in G.nodes():\n",
    "        G.nodes[node]['strategy'] = new_strategies[node]\n",
    "\n",
    "def gini_coefficient(resources):\n",
    "    sorted_resources = sorted(resources)\n",
    "    n = len(resources)\n",
    "    cumulative = np.cumsum(sorted_resources)\n",
    "    gini_index = (2 * np.sum(cumulative)) / (n * np.sum(sorted_resources)) - (n + 1) / n\n",
    "    return gini_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate():\n",
    "    G = nx.erdos_renyi_graph(NUM_AGENTS, 0.1)  # Example of a random network\n",
    "    \n",
    "    assign_resources(G, INEQUALITY_LEVEL)\n",
    "    initialize_strategies(G, INITIAL_COOPERATION_PROB)\n",
    "    \n",
    "    cooperation_levels = []\n",
    "    resource_distribution = [G.nodes[node]['resource'] for node in G.nodes()]\n",
    "\n",
    "    for t in range(NUM_TIME_STEPS):\n",
    "        payoffs = play_game(G)\n",
    "        update_strategies(G, payoffs)\n",
    "        \n",
    "        # Measure cooperation level\n",
    "        num_cooperators = sum(1 for node in G.nodes() if G.nodes[node]['strategy'] == 'C')\n",
    "        cooperation_level = num_cooperators / NUM_AGENTS\n",
    "        cooperation_levels.append(cooperation_level)\n",
    "        #print(f\"Time Step {t+1}: Cooperation Level = {cooperation_level:.2f}\")\n",
    "\n",
    "    return cooperation_levels, resource_distribution, G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 112\u001b[0m\n\u001b[0;32m    109\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Run the simulation and plot the final state\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m cooperation_levels, G \u001b[38;5;241m=\u001b[39m \u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m plot_lattice(G)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Plot the cooperation level over time\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 92\u001b[0m, in \u001b[0;36msimulate\u001b[1;34m()\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):  \u001b[38;5;66;03m# Simulate for 100 time steps\u001b[39;00m\n\u001b[0;32m     91\u001b[0m     payoffs \u001b[38;5;241m=\u001b[39m play_game(G)\n\u001b[1;32m---> 92\u001b[0m     \u001b[43mupdate_strategies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayoffs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m     num_cooperators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m G\u001b[38;5;241m.\u001b[39mnodes() \u001b[38;5;28;01mif\u001b[39;00m G\u001b[38;5;241m.\u001b[39mnodes[node][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrategy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     95\u001b[0m     cooperation_level \u001b[38;5;241m=\u001b[39m num_cooperators \u001b[38;5;241m/\u001b[39m NUM_AGENTS\n",
      "Cell \u001b[1;32mIn[2], line 75\u001b[0m, in \u001b[0;36mupdate_strategies\u001b[1;34m(G, payoffs)\u001b[0m\n\u001b[0;32m     73\u001b[0m neighbor \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(neighbors)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m payoffs[neighbor] \u001b[38;5;241m>\u001b[39m payoffs[node]:\n\u001b[1;32m---> 75\u001b[0m     prob \u001b[38;5;241m=\u001b[39m (payoffs[neighbor] \u001b[38;5;241m-\u001b[39m payoffs[node]) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mmax\u001b[39m(payoffs\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[43mpayoffs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m prob:\n\u001b[0;32m     77\u001b[0m         new_strategies[node] \u001b[38;5;241m=\u001b[39m G\u001b[38;5;241m.\u001b[39mnodes[neighbor][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrategy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "\n",
    "# Adjust grid size to 100x100\n",
    "GRID_SIZE = 100  # 100x100 lattice\n",
    "NUM_AGENTS = GRID_SIZE * GRID_SIZE  # Total number of agents = 100^2 = 10,000\n",
    "RICH_SITE_PROB = 0.2  # Fraction of rich sites\n",
    "INITIAL_COOPERATION_PROB = 0.5\n",
    "INEQUALITY_LEVEL = 10  # Resource contrast (c) between rich and poor sites\n",
    "TEMPTATION_LEVEL = 0.7  # High temptation (b-1)\n",
    "\n",
    "# Payoff matrix parameters\n",
    "RICH_RESOURCE = 1 + INEQUALITY_LEVEL\n",
    "POOR_RESOURCE = 1\n",
    "b = 2 / TEMPTATION_LEVEL  # Public goods multiplier\n",
    "\n",
    "# Create a 2D grid of agents (lattice)\n",
    "def create_lattice():\n",
    "    G = nx.grid_2d_graph(GRID_SIZE, GRID_SIZE, periodic=True)  # 2D grid with periodic boundary\n",
    "    assign_resources(G)\n",
    "    initialize_strategies(G, INITIAL_COOPERATION_PROB)\n",
    "    return G\n",
    "\n",
    "# Assign resources to rich and poor sites\n",
    "def assign_resources(G):\n",
    "    for node in G.nodes():\n",
    "        if random.random() < RICH_SITE_PROB:\n",
    "            G.nodes[node]['resource'] = RICH_RESOURCE\n",
    "            G.nodes[node]['site_type'] = 'rich'\n",
    "        else:\n",
    "            G.nodes[node]['resource'] = POOR_RESOURCE\n",
    "            G.nodes[node]['site_type'] = 'poor'\n",
    "\n",
    "# Initialize strategies for agents (either cooperate 'C' or defect 'D')\n",
    "def initialize_strategies(G, cooperation_prob):\n",
    "    for node in G.nodes():\n",
    "        G.nodes[node]['strategy'] = 'C' if random.random() < cooperation_prob else 'D'\n",
    "\n",
    "# Play the game for one time step and calculate payoffs\n",
    "def play_game(G):\n",
    "    payoffs = {}\n",
    "    for node in G.nodes():\n",
    "        strategy = G.nodes[node]['strategy']\n",
    "        resource = G.nodes[node]['resource']\n",
    "        payoff = 0\n",
    "        neighbors = G.neighbors(node)\n",
    "        \n",
    "        for neighbor in neighbors:\n",
    "            neighbor_strategy = G.nodes[neighbor]['strategy']\n",
    "            neighbor_resource = G.nodes[neighbor]['resource']\n",
    "\n",
    "            # Payoff matrix from the paper (rich and poor site interactions)\n",
    "            if strategy == 'C' and neighbor_strategy == 'C':\n",
    "                payoff += neighbor_resource  # Cooperation between cooperators\n",
    "            elif strategy == 'C' and neighbor_strategy == 'D':\n",
    "                payoff += 0  # Cooperator interacting with defector\n",
    "            elif strategy == 'D' and neighbor_strategy == 'C':\n",
    "                payoff += b * resource  # Defector interacting with cooperator\n",
    "            elif strategy == 'D' and neighbor_strategy == 'D':\n",
    "                payoff += 0  # Defector interacting with defector\n",
    "\n",
    "        payoffs[node] = payoff\n",
    "    return payoffs\n",
    "\n",
    "# Update strategies based on payoffs and stochastic imitation\n",
    "def update_strategies(G, payoffs):\n",
    "    new_strategies = {}\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        if neighbors:\n",
    "            neighbor = random.choice(neighbors)\n",
    "            if payoffs[neighbor] > payoffs[node]:\n",
    "                prob = (payoffs[neighbor] - payoffs[node]) / (max(payoffs.values()) - min(payoffs.values()) + 1e-6)\n",
    "                if random.random() < prob:\n",
    "                    new_strategies[node] = G.nodes[neighbor]['strategy']\n",
    "                else:\n",
    "                    new_strategies[node] = G.nodes[node]['strategy']\n",
    "            else:\n",
    "                new_strategies[node] = G.nodes[node]['strategy']\n",
    "    for node in G.nodes():\n",
    "        G.nodes[node]['strategy'] = new_strategies[node]\n",
    "\n",
    "# Simulation loop\n",
    "def simulate():\n",
    "    G = create_lattice()\n",
    "    cooperation_levels = []\n",
    "\n",
    "    for t in range(100):  # Simulate for 100 time steps\n",
    "        payoffs = play_game(G)\n",
    "        update_strategies(G, payoffs)\n",
    "        \n",
    "        num_cooperators = sum(1 for node in G.nodes() if G.nodes[node]['strategy'] == 'C')\n",
    "        cooperation_level = num_cooperators / NUM_AGENTS\n",
    "        cooperation_levels.append(cooperation_level)\n",
    "        \n",
    "        # Plot the lattice at time steps 1, 50, and 100\n",
    "        if t in [0, 49, 99]:\n",
    "            plot_lattice(G, t+1)\n",
    "    \n",
    "    return cooperation_levels, G\n",
    "\n",
    "# Visualization of the lattice with different colors for cooperators and defectors\n",
    "def plot_lattice(G, timestep):\n",
    "    # Color cooperators (blue) and defectors (red)\n",
    "    strategy_map = {'C': 'blue', 'D': 'red'}\n",
    "    node_colors = [strategy_map[G.nodes[node]['strategy']] for node in G.nodes()]\n",
    "    \n",
    "    # Draw the grid with proper node colors\n",
    "    pos = dict((n, n) for n in G.nodes())  # Use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(cooperation_levels, resource_distribution, G):\n",
    "    # Plot cooperation levels over time\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(NUM_TIME_STEPS), cooperation_levels, marker='o')\n",
    "    plt.title('Cooperation Level Over Time')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Cooperation Level')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot resource distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(resource_distribution, bins=20, edgecolor='black')\n",
    "    plt.title('Resource Distribution Among Agents')\n",
    "    plt.xlabel('Normalized Resource Level')\n",
    "    plt.ylabel('Number of Agents')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Visualize network with strategies\n",
    "    color_map = {'C': 'blue', 'D': 'red'}\n",
    "    node_colors = [color_map[G.nodes[node]['strategy']] for node in G.nodes()]\n",
    "    sizes = [G.nodes[node]['resource'] * 1000 for node in G.nodes()]  # Scale sizes for visualization\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    nx.draw_networkx(G, node_color=node_colors, node_size=sizes, with_labels=False)\n",
    "    plt.title('Network Visualization: Blue = Cooperators, Red = Defectors')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Resource Distribution vs Cooperation Level\n",
    "    strategies = [G.nodes[node]['strategy'] for node in G.nodes()]\n",
    "    cooperation_flags = [1 if strategy == 'C' else 0 for strategy in strategies]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(resource_distribution, cooperation_flags, c=cooperation_flags, cmap='coolwarm', alpha=0.7)\n",
    "    plt.title('Resource Distribution vs Cooperation Level')\n",
    "    plt.xlabel('Resource Level')\n",
    "    plt.ylabel('Cooperation (1 = Cooperate, 0 = Defect)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Gini Coefficient vs Cooperation Level\n",
    "    gini = gini_coefficient(resource_distribution)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(['Cooperation Level', 'Gini Coefficient'], [cooperation_levels[-1], gini])\n",
    "    plt.title('Final Cooperation Level vs Gini Coefficient (Inequality)')\n",
    "    plt.ylabel('Value')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_simulation(num_agents, cooperation_prob, inequality_level):\n",
    "    global NUM_AGENTS, INITIAL_COOPERATION_PROB, INEQUALITY_LEVEL\n",
    "    NUM_AGENTS = num_agents\n",
    "    INITIAL_COOPERATION_PROB = cooperation_prob\n",
    "    INEQUALITY_LEVEL = inequality_level\n",
    "\n",
    "    #Run & plot simulation\n",
    "    cooperation_levels, resource_distribution, G = simulate()\n",
    "    plot_results(cooperation_levels, resource_distribution, G)\n",
    "\n",
    "widgets.interact(interactive_simulation,\n",
    "                 num_agents=widgets.IntSlider(min=50, max=500, step=50, value=100),\n",
    "                 cooperation_prob=widgets.FloatSlider(min=0, max=1, step=0.1, value=0.5),\n",
    "                 inequality_level=widgets.FloatSlider(min=1, max=5, step=0.1, value=2.0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envNS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
